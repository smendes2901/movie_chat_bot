{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEOqJFudTizP","outputId":"3ee5cfa0-fe9e-449c-ddbc-f1d26a17b294","executionInfo":{"status":"ok","timestamp":1701654408330,"user_tz":300,"elapsed":21,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Dec  4 01:46:47 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    26W / 300W |      0MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHm7ZCByTkb1","outputId":"54acb530-af7e-461a-b94b-db8f5560c916","executionInfo":{"status":"ok","timestamp":1701699679925,"user_tz":300,"elapsed":212,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -Uqqq pip --progress-bar off\n","!pip install -qqq transformers datasets peft sentencepiece accelerate bitsandbytes --progress-bar off"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ogQ5EqcTT9rV","executionInfo":{"status":"ok","timestamp":1701699700546,"user_tz":300,"elapsed":20622,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[],"source":["import json\n","import re\n","from pprint import pprint\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from datasets import Dataset, DatasetDict, load_dataset\n","from huggingface_hub import notebook_login\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, get_peft_model_state_dict\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    BitsAndBytesConfig,\n","    DataCollatorForSeq2Seq\n",")\n","\n","DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kjsKDHZEigD"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWLDvo1fbocN"},"outputs":[],"source":["def create_model_and_tokenizer():\n","  compute_dtype = getattr(torch, \"bfloat16\")\n","  bnb_config = BitsAndBytesConfig(\n","          load_in_4bit=True,\n","          bnb_4bit_quant_type=\"nf4\",\n","          bnb_4bit_compute_dtype=compute_dtype,\n","          bnb_4bit_use_double_quant=True,\n","  )\n","\n","\n","  model = AutoModelForCausalLM.from_pretrained(\n","      MODEL_NAME,\n","      # use_safetensors=True,\n","      # load_in_8bit=True,\n","      # torch_dtype=torch.float16,\n","      # trust_remote_code=True,\n","      quantization_config=bnb_config,\n","      device_map=\"auto\",\n","  )\n","\n","  # model.to(DEVICE)\n","\n","  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","  tokenizer.pad_token = tokenizer.unk_token\n","  tokenizer.pad_token_id =  tokenizer.unk_token_id\n","  # tokenizer.pad_token_id = 0 # unk. we want this to be different from the eos token\n","  tokenizer.padding_side = \"left\"\n","\n","  return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1iTUvu7Hh9I"},"outputs":[],"source":["notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbCPvT6VcjlX"},"outputs":[],"source":["model, tokenizer = create_model_and_tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taqOy9tgW0L5"},"outputs":[],"source":["def tokenize_prompt(prompt, add_eos_token=True) -> dict:\n","\n","  result = tokenizer(\n","        prompt,\n","        truncation=True,\n","        max_length=1024,\n","        padding=False,\n","        return_tensors=None,\n","    )\n","  if (\n","      result[\"input_ids\"][-1] != tokenizer.eos_token_id\n","      and len(result[\"input_ids\"]) < 1024\n","      and add_eos_token\n","  ):\n","      result[\"input_ids\"].append(tokenizer.eos_token_id)\n","      result[\"attention_mask\"].append(1)\n","  result[\"labels\"] = result[\"input_ids\"].copy()\n","\n","  # result[\"input_ids\"] = torch.from_numpy(np.array(result[\"input_ids\"])).to(DEVICE)\n","  # result[\"attention_mask\"] = torch.from_numpy(np.array(result[\"attention_mask\"])).to(DEVICE)\n","  # result[\"labels\"] = torch.from_numpy(np.array(result[\"labels\"])).to(DEVICE)\n","\n","  return result\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9aGDE9WcEwIY","executionInfo":{"status":"error","timestamp":1701699645149,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}},"colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"7652714a-43d0-4ce6-84f6-fb3ef9b0a452"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-90faa771b15b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/Othercomputers/My Laptop/Desktop/Stevens/SEM1/Deep Learning - CS 583 A/CS 583 Project/datasets/movie_datasets/imdb/train_llm_ds_sm_v2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/Othercomputers/My Laptop/Desktop/Stevens/SEM1/Deep Learning - CS 583 A/CS 583 Project/datasets/movie_datasets/imdb/val_llm_ds_sm_v2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["train_data = pd.read_csv(\"/content/drive/Othercomputers/My Laptop/Desktop/Stevens/SEM1/Deep Learning - CS 583 A/CS 583 Project/datasets/movie_datasets/imdb/train_llm_ds_sm_v2.csv\")\n","val_data = pd.read_csv(\"/content/drive/Othercomputers/My Laptop/Desktop/Stevens/SEM1/Deep Learning - CS 583 A/CS 583 Project/datasets/movie_datasets/imdb/val_llm_ds_sm_v2.csv\")"]},{"cell_type":"code","source":["train_data.dropna(axis=0, inplace=True)\n","val_data.dropna(axis=0, inplace=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"L6mq0NsdM8sv","executionInfo":{"status":"error","timestamp":1701699645437,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}},"outputId":"67b522a6-1243-433a-d6ff-879695a5c97e"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-0beffc64b64b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_VnUnpwGwPF"},"outputs":[],"source":["train_ds = Dataset.from_pandas(train_data)\n","val_ds = Dataset.from_pandas(val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406,"referenced_widgets":["f3d25095d0924e52a6c2748348eb0b08","e92b82434b3b4090ab27ede4b85dd38d","c8d147e21c3c4c97928585f0b2e38ee3","4ab04ecaa37d4445bf3e41153af159b2","c64abb60d4ec4ebc9de9b78bd038d803","ea5c27c1a5f94dc1befd76b0cf095ffc","7fbae1c950c14c89b25f757a98e12f1d","c387b07e7e9e4e1aaa40848b89cce93a","6836fe130adf477f8f69fbfab5e18580","b73afff0cca34373b1b03c5049e0f37c","0362fa17fde94d6393ec1e92b2c9bcd1"]},"id":"gPI1PYLgG6mq","outputId":"4cd619c0-ac44-4e56-8714-067c32bf2410","executionInfo":{"status":"error","timestamp":1701658017820,"user_tz":300,"elapsed":2539,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d25095d0924e52a6c2748348eb0b08"}},"metadata":{}},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a58a802edb42>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         }\n\u001b[1;32m    555\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3087\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3089\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3090\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3091\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3440\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m                         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_function_on_filtered_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3344\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3345\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3347\u001b[0m                 processed_inputs = {\n","\u001b[0;32m<ipython-input-14-a58a802edb42>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sample)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-df80ea4740c8>\u001b[0m in \u001b[0;36mtokenize_prompt\u001b[0;34m(prompt, add_eos_token)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   result = tokenizer(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2790\u001b[0m         \u001b[0mall_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2792\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You need to specify either `text` or `text_target`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m             \u001b[0;31m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You need to specify either `text` or `text_target`."]}],"source":["train_ds = train_ds.map(lambda sample: tokenize_prompt(sample[\"prompt\"]))\n","val_ds = val_ds.map(lambda sample: tokenize_prompt(sample[\"prompt\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuRFuRGJa7j5"},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3fx3yP2coNJ"},"outputs":[],"source":["model.config.use_cache = False\n","model.config.quantization_config.to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I40wOuf0dY4m"},"outputs":[],"source":["LORA_R = 16\n","LORA_ALPHA = 16\n","LORA_DROPOUT= 0.05\n","LORA_TARGET_MODULES = [\n","    \"q_proj\",\n","    \"v_proj\",\n","    \"k_proj\",\n","    \"o_proj\",\n","    \"gate_proj\",\n","    \"down_proj\",\n","    \"up_proj\"\n","]\n","\n","BATCH_SIZE = 128\n","MICRO_BATCH_SIZE = 8\n","GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n","LEARNING_RATE = 2e-4\n","# TRAIN_STEPS = 100\n","OUTPUT_DIR = \"experiments\"\n","\n","config = LoraConfig(\n","    r=LORA_R,\n","    lora_alpha=LORA_ALPHA,\n","    target_modules=LORA_TARGET_MODULES,\n","    lora_dropout=LORA_DROPOUT,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, config)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HveOm6yUeGQK"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir experiments/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmzK45Ureo2o"},"outputs":[],"source":["training_arguments = TrainingArguments(\n","    per_device_train_batch_size=MICRO_BATCH_SIZE,\n","    gradient_accumulation_steps=MICRO_BATCH_SIZE,\n","    lr_scheduler_type=\"linear\",\n","    # max_steps=1000,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=1,\n","    learning_rate=LEARNING_RATE,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=20,\n","    save_steps=20,\n","    warmup_ratio=0.05,\n","    save_strategy=\"steps\",\n","    group_by_length=True,\n","    output_dir=OUTPUT_DIR,\n","    report_to=\"tensorboard\",\n","    save_safetensors=True,\n","    seed=42,\n","    load_best_model_at_end=True,\n","    num_train_epochs=1,\n","    neftune_noise_alpha=0.1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewz_O_Sld2bP"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    max_length=1024,\n","    pad_to_multiple_of=8,\n","    return_tensors=\"pt\",\n","    padding=True,\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69Hvbr4kgYwh"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    args=training_arguments,\n","    data_collator=data_collator,\n","    # neftune_noise_alpha=0.1,\n",")\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"id":"0Z4d-DQWguXg","outputId":"c2730644-f62a-4558-8ec1-a15d6ab80e58","executionInfo":{"status":"ok","timestamp":1701657176453,"user_tz":300,"elapsed":942046,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [31/31 15:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.231400</td>\n","      <td>1.365619</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.452400</td>\n","      <td>1.151991</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.063600</td>\n","      <td>1.113009</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]}],"source":["with torch.autocast(\"cuda\"):\n","  trainer.train()"]},{"cell_type":"code","source":["model.save_pretrained(\"/content/drive/Othercomputers/My Laptop/Desktop/Stevens/SEM1/Deep Learning - CS 583 A/CS 583 Project/model_dump/llama-7b-v1/\", safe_serialization=False)"],"metadata":{"id":"uCK5KYCOtISl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vogn9b-Sdz1h","outputId":"51d166ea-bcd4-4c73-cdac-cc23bbe14b6f"},"outputs":[{"ename":"NotImplementedError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-b46b559ecd3b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp -r experiments/ /content/drive/MyDrive/Text\\\\ Mining\\\\ and\\\\ Information\\\\ Retrieval\\\\ -\\\\ CS\\\\ 589\\\\ A/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n","\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"]}],"source":["!cp -r experiments/ /content/drive/MyDrive/Text\\ Mining\\ and\\ Information\\ Retrieval\\ -\\ CS\\ 589\\ A/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFk-7WV2d4Y8","outputId":"284e1c0b-bd5d-4304-b143-2eafb35b56d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["drive  experiments  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3-kAzGhOP8i"},"outputs":[],"source":["model.save_pretrained(\"/content/drive/MyDrive/Text Mining and Information Retrieval - CS 589 A/meta-llama-Llama-2-7b-hf/\", safe_serialization=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sMHfgTRdf1W"},"outputs":[],"source":["test_data = pd.read_csv(\"/content/drive/Othercomputers/My Laptop/Desktop/Stevens/SEM1/Deep Learning - CS 583 A/CS 583 Project/datasets/movie_datasets/imdb/test_llm_ds_vsm_v2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iT3li4iPdbpV","outputId":"adc751c3-d959-4ac3-f33e-625ba30b9e08","executionInfo":{"status":"ok","timestamp":1701657454834,"user_tz":300,"elapsed":4,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Below is a question regarding movies and shows paired with an input that provides further context. Write a response that appropriately completes the request.\n","###Instruction: What is the main conflict in the movie “Ang henerasyong sumuko sa love”?\n","###Input: Description: It tells the story of different teenagers who have different priorities and encounter different challenges as they enter adult life. \n","Release Year: 2019 \n","Runtime(in minutes): 100 \n","Genre: Drama,Romance \n","Rating: 6.5 \n","Votes: 71.0\n","###Response:\n"]}],"source":["idx = 14\n","prompt = test_data.iloc[idx].prompt\n","print(prompt)"]},{"cell_type":"code","source":["prompt = prompt.split(\"###Response:\")[0]+\"###Response:\"\n","prompt = prompt.strip()\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bh2pgrsboLZi","executionInfo":{"status":"ok","timestamp":1701657465545,"user_tz":300,"elapsed":551,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}},"outputId":"a98ab354-36ab-495e-e2a0-8825516a3c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is a question regarding movies and shows paired with an input that provides further context. Write a response that appropriately completes the request.\n","###Instruction: What is the main conflict in the movie “Ang henerasyong sumuko sa love”?\n","###Input: Description: It tells the story of different teenagers who have different priorities and encounter different challenges as they enter adult life. \n","Release Year: 2019 \n","Runtime(in minutes): 100 \n","Genre: Drama,Romance \n","Rating: 6.5 \n","Votes: 71.0\n","###Response:\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8RE5TX0RjhH"},"outputs":[],"source":["inputs = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = inputs[\"input_ids\"].to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCp12wuwwvCm"},"outputs":[],"source":["with torch.no_grad():\n","    generation_output = trainer.model.generate(\n","                    input_ids=input_ids,\n","                    return_dict_in_generate=True,\n","                    output_scores=True,\n","                    max_new_tokens=512,\n","                )\n","s = generation_output.sequences[0]\n","output = tokenizer.decode(s, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RVBb85kxDB9","outputId":"fca2152a-51d6-44d4-8846-36ceaef306a7","executionInfo":{"status":"ok","timestamp":1701657477037,"user_tz":300,"elapsed":6,"user":{"displayName":"Shaun Mendes","userId":"17743155916172566182"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Below is a question regarding movies and shows paired with an input that provides further context. Write a response that appropriately completes the request.\n","###Instruction: What is the main conflict in the movie “Ang henerasyong sumuko sa love”?\n","###Input: Description: It tells the story of different teenagers who have different priorities and encounter different challenges as they enter adult life. \n","Release Year: 2019 \n","Runtime(in minutes): 100 \n","Genre: Drama,Romance \n","Rating: 6.5 \n","Votes: 71.0\n","###Response: The main conflict in the movie “Ang henerasyong sumuko sa love” is the conflict between the teenagers’ priorities and the challenges they encounter as they enter adult life.\n"]}],"source":["print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vm6rbKdMxbmD"},"outputs":[],"source":["trainer.model.save_pretrained(\"/content/drive/MyDrive/Shaun/openlm-research-open_llama_3b_v2-pre/\", save_adapter=True, save_config=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3F4xbZ7ZFppT"},"outputs":[],"source":["trainer.save_model(\"/content/drive/MyDrive/Shaun/openlm-research-open_llama_3b_v2/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUY8w16hGwvT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f3d25095d0924e52a6c2748348eb0b08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e92b82434b3b4090ab27ede4b85dd38d","IPY_MODEL_c8d147e21c3c4c97928585f0b2e38ee3","IPY_MODEL_4ab04ecaa37d4445bf3e41153af159b2"],"layout":"IPY_MODEL_c64abb60d4ec4ebc9de9b78bd038d803"}},"e92b82434b3b4090ab27ede4b85dd38d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea5c27c1a5f94dc1befd76b0cf095ffc","placeholder":"​","style":"IPY_MODEL_7fbae1c950c14c89b25f757a98e12f1d","value":"Map:  68%"}},"c8d147e21c3c4c97928585f0b2e38ee3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_c387b07e7e9e4e1aaa40848b89cce93a","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6836fe130adf477f8f69fbfab5e18580","value":3417}},"4ab04ecaa37d4445bf3e41153af159b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b73afff0cca34373b1b03c5049e0f37c","placeholder":"​","style":"IPY_MODEL_0362fa17fde94d6393ec1e92b2c9bcd1","value":" 3417/5000 [00:02&lt;00:00, 1602.46 examples/s]"}},"c64abb60d4ec4ebc9de9b78bd038d803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5c27c1a5f94dc1befd76b0cf095ffc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fbae1c950c14c89b25f757a98e12f1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c387b07e7e9e4e1aaa40848b89cce93a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6836fe130adf477f8f69fbfab5e18580":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b73afff0cca34373b1b03c5049e0f37c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0362fa17fde94d6393ec1e92b2c9bcd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}